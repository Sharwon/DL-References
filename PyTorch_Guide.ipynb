{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to PyTorch !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "PyTorch is a python framework for deep learning tasks. It was tailored to be fast and pythnonic(Yeah!). The biggest                    advantage is its ability to automatically calculate gradients for the specified variables.The autograd package provides automatic differentiation for all operations on variables.This is very important in  case of deep learning, as calculating gradients during back-propogation becomes hassle free.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-21T10:24:27.762097Z",
     "start_time": "2018-03-21T10:24:25.846446Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "#import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor \n",
    "\n",
    "it's a n-diamensional array which resides on the gpu(mostly of the cases).\n",
    "\n",
    "Types supported:\n",
    "\n",
    "    64-bit (Float + Int)\n",
    "    32-bit (Float + Int)\n",
    "    16-bit (Float + Int)\n",
    "    8-bit (Signed + Unsigned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-21T10:24:36.598266Z",
     "start_time": "2018-03-21T10:24:36.585417Z"
    }
   },
   "outputs": [],
   "source": [
    "#Creating a 2x3 tensor.\n",
    "x = torch.Tensor(2, 3)\n",
    "\n",
    "#Creating a 2x3 tensor with values randomly selected from a Uniform Distribution between -1 and 1\n",
    "y = torch.Tensor(2, 3)\n",
    "y = y.uniform_(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-21T10:24:37.953426Z",
     "start_time": "2018-03-21T10:24:37.944616Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-0.7006  0.1955 -0.2596\n",
      "-0.7500  0.8722  0.5685\n",
      "[torch.FloatTensor of size 2x3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-21T10:24:47.267890Z",
     "start_time": "2018-03-21T10:24:47.239793Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "-0.4717  1.0220 -1.2458\n",
       " 0.2018  0.1323  0.4026\n",
       "[torch.FloatTensor of size 2x3]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add and storing result in another tensor.\n",
    "x.uniform_(-1, 1)\n",
    "result = torch.Tensor(1, 1)\n",
    "torch.add(x, y, out=result)\n",
    "# notice how result got broadcasted into an tensor of 2x3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-21T10:18:09.167638Z",
     "start_time": "2018-03-21T10:18:09.162849Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-0.6264 -1.7384 -0.4581\n",
      " 1.3804  1.5894  0.5036\n",
      "[torch.FloatTensor of size 2x3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# normal multiplication\n",
    "print(torch.mul(y, 2.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-21T10:18:10.854295Z",
     "start_time": "2018-03-21T10:18:10.540195Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "-0.6264 -1.7384 -0.4581\n",
       " 1.3804  1.5894  0.5036\n",
       "[torch.FloatTensor of size 2x3]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using In-line functions.. not the '_', helps in faster execution time. Here adding y with itself and storing \n",
    "y.add_(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-21T10:18:13.704065Z",
     "start_time": "2018-03-21T10:18:13.695660Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.6263864  -1.7383876  -0.45812273]\n",
      " [ 1.3803585   1.5894313   0.50355387]]\n"
     ]
    }
   ],
   "source": [
    "#Converting Tensors to Numpy arrays.\n",
    "nampy = y.numpy()\n",
    "print(nampy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-21T10:18:24.317389Z",
     "start_time": "2018-03-21T10:18:15.780611Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-1.2528 -3.4768 -0.9162\n",
      " 2.7607  3.1789  1.0071\n",
      "[torch.cuda.FloatTensor of size 2x3 (GPU 0)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#moving the whole operations to GPU.\n",
    "if torch.cuda.is_available():\n",
    "    #y = y.cuda()\n",
    "    u = (y + y).cuda()\n",
    "print(u) # notice (GPU0) at the end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Variable\n",
    " __autograd.Variable__ is the central class of the package. It wraps a Tensor, and supports nearly all of operations defined on it. Once you finish your computation you can call __.backward()__ and have all the gradients computed automatically.\n",
    "\n",
    "![alt text](http://pytorch.org/tutorials/_images/Variable.png \"Variable Structure\")\n",
    "\n",
    "You can access the raw tensor through the __.data__ attribute, while the gradient w.r.t. this variable is accumulated into __.grad__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# import Variable from pytorch.\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Creating a dot product of two matrices.\n",
    "\n",
    "x = Variable(torch.cuda.FloatTensor([10, 10]))\n",
    "y = Variable(torch.cuda.FloatTensor([5, 0]), requires_grad=True)\n",
    "\n",
    "z = x.dot(y*y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "value of z : \n",
      " 250\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "z.backward(retain_graph=True) # for computing gradients automatically.\n",
    "print(f'value of z : {z.data}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 100\n",
       "   0\n",
       "[torch.cuda.FloatTensor of size 2 (GPU 0)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.grad.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#runing backward pass for the second time.\n",
    "z.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 200\n",
       "   0\n",
       "[torch.cuda.FloatTensor of size 2 (GPU 0)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.grad.data\n",
    "#Here the resultant gradient is erroneous according to our actual input.\n",
    "#This is because while, calculating the gradiets during the second pass, they get added with the gradients from the first pass.\n",
    "#Initializing weights to zero after each pass, solves the issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradients form the first run : \n",
      " 100\n",
      "   0\n",
      "[torch.cuda.FloatTensor of size 2 (GPU 0)]\n",
      "\n",
      "Gradients form the first run : \n",
      " 200\n",
      "   0\n",
      "[torch.cuda.FloatTensor of size 2 (GPU 0)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = Variable(torch.cuda.FloatTensor([10, 10]))\n",
    "y = Variable(torch.cuda.FloatTensor([5, 0]), requires_grad=True)\n",
    "z = x.dot(y*y)\n",
    "\n",
    "z.backward(retain_graph=True)\n",
    "print(f'Gradients form the first run : {y.grad.data}')\n",
    "\n",
    "#Uncomment the line below to understand the error.\n",
    "#y.grad.data.zero_() # weights --> 0\n",
    "\n",
    "z.backward()\n",
    "print(f'Gradients form the first run : {y.grad.data}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict (before training) 4 4.0\n",
      "\tgrad:  1.0 2.0 -2.0\n",
      "\tgrad:  2.0 4.0 -7.840000152587891\n",
      "\tgrad:  3.0 6.0 -16.228801727294922\n",
      "progress: 0 7.315943717956543\n",
      "\tgrad:  1.0 2.0 -1.478623867034912\n",
      "\tgrad:  2.0 4.0 -5.796205520629883\n",
      "\tgrad:  3.0 6.0 -11.998146057128906\n",
      "progress: 1 3.9987640380859375\n",
      "\tgrad:  1.0 2.0 -1.0931644439697266\n",
      "\tgrad:  2.0 4.0 -4.285204887390137\n",
      "\tgrad:  3.0 6.0 -8.870372772216797\n",
      "progress: 2 2.1856532096862793\n",
      "\tgrad:  1.0 2.0 -0.8081896305084229\n",
      "\tgrad:  2.0 4.0 -3.1681032180786133\n",
      "\tgrad:  3.0 6.0 -6.557973861694336\n",
      "progress: 3 1.1946394443511963\n",
      "\tgrad:  1.0 2.0 -0.5975041389465332\n",
      "\tgrad:  2.0 4.0 -2.3422164916992188\n",
      "\tgrad:  3.0 6.0 -4.848389625549316\n",
      "progress: 4 0.6529689431190491\n",
      "\tgrad:  1.0 2.0 -0.4417421817779541\n",
      "\tgrad:  2.0 4.0 -1.7316293716430664\n",
      "\tgrad:  3.0 6.0 -3.58447265625\n",
      "progress: 5 0.35690122842788696\n",
      "\tgrad:  1.0 2.0 -0.3265852928161621\n",
      "\tgrad:  2.0 4.0 -1.2802143096923828\n",
      "\tgrad:  3.0 6.0 -2.650045394897461\n",
      "progress: 6 0.195076122879982\n",
      "\tgrad:  1.0 2.0 -0.24144840240478516\n",
      "\tgrad:  2.0 4.0 -0.9464778900146484\n",
      "\tgrad:  3.0 6.0 -1.9592113494873047\n",
      "progress: 7 0.10662525147199631\n",
      "\tgrad:  1.0 2.0 -0.17850565910339355\n",
      "\tgrad:  2.0 4.0 -0.699742317199707\n",
      "\tgrad:  3.0 6.0 -1.4484672546386719\n",
      "progress: 8 0.0582793727517128\n",
      "\tgrad:  1.0 2.0 -0.1319713592529297\n",
      "\tgrad:  2.0 4.0 -0.5173273086547852\n",
      "\tgrad:  3.0 6.0 -1.070866584777832\n",
      "progress: 9 0.03185431286692619\n",
      "predict (after training) 4 7.804864406585693\n"
     ]
    }
   ],
   "source": [
    "x_data = [1.0, 2.0, 3.0]\n",
    "y_data = [2.0, 4.0, 6.0]\n",
    "\n",
    "w = Variable(torch.Tensor([1.0]),  requires_grad=True)  # Any random value\n",
    "\n",
    "# our model forward pass\n",
    "def forward(x):\n",
    "    return x * w\n",
    "\n",
    "# Loss function\n",
    "def loss(x, y):\n",
    "    y_pred = forward(x)\n",
    "    return (y_pred - y) * (y_pred - y)\n",
    "\n",
    "# Before training\n",
    "print(\"predict (before training)\",  4, forward(4).data[0])\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(10):\n",
    "    for x_val, y_val in zip(x_data, y_data):\n",
    "        l = loss(x_val, y_val)\n",
    "        l.backward()\n",
    "        print(\"\\tgradient: \", x_val, y_val, w.grad.data[0])\n",
    "        w.data = w.data - 0.01 * w.grad.data\n",
    "\n",
    "        # Manually zero the gradients after updating weights\n",
    "        w.grad.data.zero_()\n",
    "\n",
    "    print(\"progress:\", epoch, l.data[0])\n",
    "\n",
    "# After training\n",
    "print(\"predict (after training)\", 4, forward(4).data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# data loader form pytorch library\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "##import numpy as np\n",
    "#datasets that are already in given in the library.\n",
    "import torchvision.datasets as datasets\n",
    "#tranforms that can be applied to the dataset. Like, 'ToTensor()' as it will be in a processable format.\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if our data is already preprocessed into binary code library. We can invoke it using the default dataloaders. ....MNIST Dataset\n",
    "train_dataset = datasets.MNIST(root='./data/data',\n",
    "                               train=True,\n",
    "                               transform=transforms.ToTensor(),\n",
    "                               download=True)\n",
    "\n",
    "test_dataset = datasets.MNIST(root='./data/data',\n",
    "                              train=False,\n",
    "                              transform=transforms.ToTensor())\n",
    "\n",
    "# Data Loader (Input Pipeline)\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "(0 ,0 ,.,.) = \n",
      "  0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
      "  0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
      "  0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
      "           ...             ⋱             ...          \n",
      "  0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
      "  0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
      "  0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
      "     ⋮ \n",
      "\n",
      "(1 ,0 ,.,.) = \n",
      "  0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
      "  0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
      "  0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
      "           ...             ⋱             ...          \n",
      "  0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
      "  0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
      "  0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
      "     ⋮ \n",
      "\n",
      "(2 ,0 ,.,.) = \n",
      "  0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
      "  0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
      "  0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
      "           ...             ⋱             ...          \n",
      "  0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
      "  0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
      "  0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
      "...   \n",
      "     ⋮ \n",
      "\n",
      "(29,0 ,.,.) = \n",
      "  0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
      "  0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
      "  0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
      "           ...             ⋱             ...          \n",
      "  0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
      "  0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
      "  0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
      "     ⋮ \n",
      "\n",
      "(30,0 ,.,.) = \n",
      "  0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
      "  0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
      "  0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
      "           ...             ⋱             ...          \n",
      "  0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
      "  0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
      "  0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
      "     ⋮ \n",
      "\n",
      "(31,0 ,.,.) = \n",
      "  0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
      "  0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
      "  0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
      "           ...             ⋱             ...          \n",
      "  0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
      "  0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
      "  0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
      "[torch.FloatTensor of size 32x1x28x28]\n",
      ", \n",
      " 7\n",
      " 5\n",
      " 0\n",
      " 6\n",
      " 9\n",
      " 1\n",
      " 4\n",
      " 8\n",
      " 7\n",
      " 9\n",
      " 0\n",
      " 0\n",
      " 6\n",
      " 6\n",
      " 2\n",
      " 0\n",
      " 1\n",
      " 5\n",
      " 0\n",
      " 3\n",
      " 8\n",
      " 4\n",
      " 4\n",
      " 2\n",
      " 9\n",
      " 6\n",
      " 8\n",
      " 8\n",
      " 4\n",
      " 8\n",
      " 4\n",
      " 3\n",
      "[torch.LongTensor of size 32]\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "for i in train_loader:\n",
    "    print(i)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a generator class, choose any dataset that you like.\n",
    "\n",
    "class MyDataLoader(Dataset):\n",
    "    #Initialize your dataset, like loading it.\n",
    "    \n",
    "    def __init__(self, filename = 'data/shakespeare.txt'):\n",
    "        self.len = 0\n",
    "        with open(filename, mode='rt') as f:\n",
    "            self.ylines = [x.strip() for x in f if x.strip()]\n",
    "            self.slines = [x.lower() for x in self.ylines]\n",
    "            self.len = len(self.slines)    \n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.slines[index], self.ylines[index]\n",
    "            \n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 data (\"i' the morn; i'll call you at your house: give\", \"i'll wait upon your lordship.\", 'that most are willing. if any such be here--', 'and well she can persuade.')\n"
     ]
    }
   ],
   "source": [
    "# invoking our custom dataset loader\n",
    "dataset = MyDataLoader()\n",
    "train_loader = DataLoader(dataset=dataset,\n",
    "                          batch_size=4,\n",
    "                          shuffle=True,\n",
    "                          num_workers=2)\n",
    "\n",
    "for i, (src, target) in enumerate(train_loader):\n",
    "    print(i, \"data\", src)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pytorch text-loader, a bit more intricate than others.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Neural Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "# Hyper Parameters \n",
    "input_size = 784\n",
    "hidden_size = 500\n",
    "num_classes = 10\n",
    "num_epochs = 5\n",
    "batch_size = 100\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'datasets' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-3afa4fcb02c7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# MNIST Dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m train_dataset = datasets.MNIST(root='./data/',\n\u001b[0m\u001b[1;32m      3\u001b[0m                                \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                                \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mToTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                                download=True)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'datasets' is not defined"
     ]
    }
   ],
   "source": [
    "# MNIST Dataset\n",
    "train_dataset = datasets.MNIST(root='./data/',\n",
    "                               train=True,\n",
    "                               transform=transforms.ToTensor(),\n",
    "                               download=True)\n",
    "\n",
    "test_dataset = datasets.MNIST(root='./data/',\n",
    "                              train=False,\n",
    "                              transform=transforms.ToTensor())\n",
    "\n",
    "# Data Loader (Input Pipeline)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.mp = nn.MaxPool2d(2)\n",
    "        self.fc = nn.Linear(320, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        in_size = x.size(0)\n",
    "        x = F.relu(self.mp(self.conv1(x)))\n",
    "        x = F.relu(self.mp(self.conv2(x)))\n",
    "        x = x.view(in_size, -1)  # flatten the tensor\n",
    "        x = self.fc(x)\n",
    "return F.log_softmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 10 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.data[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def test():\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        data, target = Variable(data, volatile=True), Variable(target)\n",
    "        output = model(data)\n",
    "        # sum up batch loss\n",
    "        test_loss += F.nll_loss(output, target, size_average=False).data[0]\n",
    "        # get the index of the max log-probability\n",
    "        pred = output.data.max(1, keepdim=True)[1]\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset), 100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for epoch in range(1, 10):\n",
    "    train(epoch)\n",
    "    test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
